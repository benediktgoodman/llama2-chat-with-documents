{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download embedding and indeference models models to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "#from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "\n",
    "# Activate cuda if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Paths for prject\n",
    "DB_DIR: str = Path.cwd().joinpath('vectorstore.db')\n",
    "HF_CACHE = Path.cwd().joinpath('model_cache')\n",
    "\n",
    "if not HF_CACHE.exists():\n",
    "    HF_CACHE.mkdir()\n",
    "\n",
    "# Make os path var as well because langchain cant handle Pathlib paths >:(\n",
    "HF_CACHE_W_PATH = os.getcwd() + \"\\model_cache\"\n",
    "\n",
    "\n",
    "# EMBEDDING_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#EMBEDDING_MODEL = \"intfloat/e5-mistral-7b-instruct\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models into specific cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\llm-sandbox\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={\"device\": device},\n",
    "    cache_folder = HF_CACHE_W_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loaders for different file types\n",
    "pdf_loader = DirectoryLoader(\"data/\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "markdown_loader = DirectoryLoader(\n",
    "    \"data/\", glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader\n",
    ")\n",
    "text_loader = DirectoryLoader(\"data/\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "all_loaders = [\n",
    "    pdf_loader, \n",
    "    markdown_loader, \n",
    "    text_loader\n",
    "    ]\n",
    "\n",
    "# Load documents from all loaders\n",
    "loaded_documents = []\n",
    "for loader in all_loaders:\n",
    "    loaded_documents.extend(loader.load())\n",
    "\n",
    "# Split loaded documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=40)\n",
    "chunked_documents = text_splitter.split_documents(loaded_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create and persist a Chroma vector database from the chunked documents\n",
    "vector_database = Chroma.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=huggingface_embeddings,\n",
    "    persist_directory=DB_DIR.as_posix(),\n",
    ")\n",
    "\n",
    "vector_database.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
