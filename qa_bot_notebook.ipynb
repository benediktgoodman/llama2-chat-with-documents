{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "\n",
    "import chainlit as cl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "\n",
    "# Activate cuda if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Paths for project\n",
    "DB_DIR: str = Path.cwd().joinpath('vectorstore.db')\n",
    "HF_CACHE = Path.cwd().joinpath('model_cache')\n",
    "\n",
    "if not HF_CACHE.exists():\n",
    "    HF_CACHE.mkdir()\n",
    "    \n",
    "# Make os path var as well because langchain cant handle Pathlib paths >:(\n",
    "HF_CACHE_W_PATH = os.getcwd() + \"\\model_cache\"\n",
    "EMBEDDING_MODEL = \"all-mpnet-base-v2\"\n",
    "INFERENCE_MODEL = \"model_cachemodels--bardsai--jaskier-7b-dpo-v5.6\"\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the users question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "The \"SOURCES\" part should be a reference to the source of the document from which you got your answer.\n",
    "The example of your response should be:\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt(prompt_template):\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def load_model(\n",
    "    model_path=\"model_cache/mistral-7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    model_type=\"mistral\",\n",
    "    max_new_tokens=983,\n",
    "    temperature=0.7,\n",
    "    gpu_layers=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a locally downloaded model.\n",
    "\n",
    "    Parameters:\n",
    "        model_path (str): The path to the model to be loaded.\n",
    "        model_type (str): The type of the model.\n",
    "        max_new_tokens (int): The maximum number of new tokens for the model.\n",
    "        temperature (float): The temperature parameter for the model.\n",
    "\n",
    "    Returns:\n",
    "        CTransformers: The loaded model.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the model file does not exist.\n",
    "        SomeOtherException: If the model file is corrupt.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"No model file found at {model_path}\")\n",
    "\n",
    "    llm = CTransformers(\n",
    "        model=model_path,\n",
    "        model_type=model_type,\n",
    "        max_new_tokens=max_new_tokens,  # type: ignore\n",
    "        temperature=temperature,  # type: ignore\n",
    "        gpu_layers=gpu_layers,\n",
    "        config = {'context_length' : 2048}\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "def get_vectorstore(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_cache=HF_CACHE_W_PATH,\n",
    "    persist_dir=\"vectorstore.db\",\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    This function creates a retrieval-based question-answering bot.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): The name of the model to be used for embeddings.\n",
    "        persist_dir (str): The directory to persist the database.\n",
    "        device (str): The device to run the model on (e.g., 'cpu', 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        RetrievalQA: The retrieval-based question-answering bot.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the persist directory does not exist.\n",
    "        SomeOtherException: If there is an issue with loading the embeddings or the model.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(persist_dir):\n",
    "        raise FileNotFoundError(f\"No directory found at {persist_dir}\")\n",
    "\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={\"device\": device},\n",
    "            cache_folder = HF_CACHE_W_PATH\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Failed to load embeddings with model name {model_name}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "    \n",
    "    return db\n",
    "\n",
    "def create_retrieval_qa_chain(llm, prompt, db, num_matches=3):\n",
    "    \"\"\"\n",
    "    Creates a Retrieval Question-Answering (QA) chain using a given language model, prompt, and database.\n",
    "\n",
    "    This function initializes a RetrievalQA object with a specific chain type and configurations,\n",
    "    and returns this QA chain. The retriever is set up to return the top 3 results (k=3).\n",
    "\n",
    "    Args:\n",
    "        llm (any): The language model to be used in the RetrievalQA.\n",
    "        prompt (str): The prompt to be used in the chain type.\n",
    "        db (any): The database to be used as the retriever.\n",
    "\n",
    "    Returns:\n",
    "        RetrievalQA: The initialized QA chain.\n",
    "    \"\"\"\n",
    "    # Gives the QAbot memory\n",
    "    message_history = ChatMessageHistory()\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        chat_memory=message_history,\n",
    "        return_messages=True,\n",
    "    )\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": num_matches}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        memory=memory,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    # files = None\n",
    "\n",
    "    # # Wait for the user to upload a file\n",
    "    # while files is None:\n",
    "    #     files = await cl.AskFileMessage(\n",
    "    #         content=\"Please upload a text file to begin!\",\n",
    "    #         accept=[\"text/plain\"],\n",
    "    #         max_size_mb=20,\n",
    "    #         timeout=180,\n",
    "    #     ).send()\n",
    "\n",
    "    # file = files[0]\n",
    "\n",
    "    # msg = cl.Message(content=f\"Processing `{file.name}`...\", disable_feedback=True)\n",
    "    # await msg.send()\n",
    "\n",
    "    # with open(file.path, \"r\", encoding=\"utf-8\") as f:\n",
    "    #     text = f.read()\n",
    "\n",
    "    # # Split the text into chunks\n",
    "    # texts = text_splitter.split_text(text)\n",
    "\n",
    "    # # Create a metadata for each chunk\n",
    "    # metadatas = [{\"source\": f\"{i}-pl\"} for i in range(len(texts))]\n",
    "\n",
    "    # # Create a Chroma vector store\n",
    "    # embeddings = HuggingFaceEmbeddings(\n",
    "    #     model_name=EMBEDDING_MODEL,\n",
    "    #     model_kwargs={\"device\": device},\n",
    "    #     cache_folder = HF_CACHE_W_PATH,\n",
    "        \n",
    "    #     )\n",
    "    \n",
    "    # docsearch = await cl.make_async(Chroma.from_texts)(\n",
    "    #     texts, embeddings, metadatas=metadatas\n",
    "    # )\n",
    "\n",
    "    db = get_vectorstore()\n",
    "    \n",
    "    message_history = ChatMessageHistory()\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        chat_memory=message_history,\n",
    "        return_messages=True,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        llm = load_model()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load model: {str(e)}\")\n",
    "    \n",
    "    # Set instructions for model\n",
    "    prompt = (\n",
    "        set_custom_prompt(prompt_template)\n",
    "    )\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        #retriever=docsearch.as_retriever(),\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    # # Let the user know that the system is ready\n",
    "    # msg.content = f\"Processing `{file.name}` done. You can now ask questions!\"\n",
    "    # await msg.update()\n",
    "\n",
    "    #print(qa_chain.keys())\n",
    "    \n",
    "    cl.user_session.set(\"chain\", qa_chain)\n",
    "    \n",
    "# @cl.on_message\n",
    "# async def main(message: cl.Message):\n",
    "#     chain = cl.user_session.get(\"chain\")  # type: ConversationalRetrievalChain\n",
    "#     cb = cl.AsyncLangchainCallbackHandler()\n",
    "\n",
    "#     res = await chain.acall(message.content, callbacks=[cb])\n",
    "#     answer = res[\"answer\"]\n",
    "#     source_documents = res[\"source_documents\"]  # type: List[Document]\n",
    "\n",
    "#     text_elements = []  # type: List[cl.Text]\n",
    "\n",
    "#     if source_documents:\n",
    "#         for source_idx, source_doc in enumerate(source_documents):\n",
    "#             source_name = f\"source_{source_idx}\"\n",
    "#             # Create the text element referenced in the message\n",
    "#             text_elements.append(\n",
    "#                 cl.Text(content=source_doc.page_content, name=source_name)\n",
    "#             )\n",
    "#         source_names = [text_el.name for text_el in text_elements]\n",
    "\n",
    "#         if source_names:\n",
    "#             answer += f\"\\nSources: {', '.join(source_names)}\"\n",
    "#         else:\n",
    "#             answer += \"\\nNo sources found\"\n",
    "\n",
    "#     await cl.Message(content=answer, elements=text_elements).send()\n",
    "    \n",
    "\n",
    "@cl.on_message\n",
    "async def process_chat_message(message):\n",
    "    \"\"\"\n",
    "    Processes incoming chat messages.\n",
    "\n",
    "    This asynchronous function retrieves the QA bot instance from the user's session,\n",
    "    sets up a callback handler for the bot's response, and executes the bot's\n",
    "    call method with the given message and callback. The bot's answer and source\n",
    "    documents are then extracted from the response.\n",
    "    \"\"\"\n",
    "    chain = cl.user_session.get(\"chain\")  # type: ConversationalRetrievalChain\n",
    "    cb = cl.AsyncLangchainCallbackHandler()\n",
    "\n",
    "    response = await chain.acall(message.content, callbacks=[cb])\n",
    "    \n",
    "    bot_answer = response[\"result\"]\n",
    "    #bot_answer = response[\"output\"]\n",
    "    source_documents = response[\"source_documents\"]\n",
    "    \n",
    "    text_elements = []\n",
    "    \n",
    "    if source_documents:\n",
    "        for source_idx, source_doc in enumerate(source_documents):\n",
    "            source_name = f\"source_{source_idx}\"\n",
    "            # Create the text element referenced in the message\n",
    "            text_elements.append(\n",
    "                cl.Text(content=source_doc.page_content, name=source_name)\n",
    "            )\n",
    "        source_names = [text_el.name for text_el in text_elements]\n",
    "\n",
    "        if source_names:\n",
    "            bot_answer += f\"\\nSources: {', '.join(source_names)}\"\n",
    "        else:\n",
    "            bot_answer += \"\\nNo sources found\"\n",
    "\n",
    "    await cl.Message(content=bot_answer).send()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
